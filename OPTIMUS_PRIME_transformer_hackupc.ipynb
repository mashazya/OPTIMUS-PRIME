{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optimus Prime x McKinsey & Company\n### Alejandro Campayo, √Ålvaro Domingo, Victor Sainz, Maria Zyatyugina","metadata":{}},{"cell_type":"markdown","source":"This code aims to predict the future amount of sales of a particular products given a speciefic date. To face this challenge, Optimus Prime team trained a Multihead Transformer model with 5 heads. The model uses the time series to find trends within the given dataset. All sales are assumed to have a weekly relation, for example, to predict an event happenning on Monday the algorithm will put attention on 7 previous days with self-attention algorithm.","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport array\nimport pickle\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport math\nimport datetime","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.417709,"end_time":"2021-02-17T13:02:53.693665","exception":false,"start_time":"2021-02-17T13:02:52.275956","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:00.837264Z","iopub.execute_input":"2022-05-01T02:32:00.837856Z","iopub.status.idle":"2022-05-01T02:32:00.844241Z","shell.execute_reply.started":"2022-05-01T02:32:00.837803Z","shell.execute_reply":"2022-05-01T02:32:00.842861Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Import datasets","metadata":{}},{"cell_type":"markdown","source":"To execute this code, please, specify the path to the files on your machine","metadata":{}},{"cell_type":"code","source":"geo_params = pd.read_csv(\"../input/mckinsey/geo_params.csv\")\ntest = pd.read_csv(\"../input/mckinsey/test.csv\")\nsku = pd.read_csv(\"../input/mckinsey/sku.csv\")\nsales = pd.read_csv(\"../input/mckinsey/sales.csv\")\nsales_mat = np.load(\"../input/sales-matrix/sales_expanded.npy\")[:,:,:,1]","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:32:00.863627Z","iopub.execute_input":"2022-05-01T02:32:00.863989Z","iopub.status.idle":"2022-05-01T02:32:05.702811Z","shell.execute_reply.started":"2022-05-01T02:32:00.863954Z","shell.execute_reply":"2022-05-01T02:32:05.701780Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Model definition","metadata":{}},{"cell_type":"code","source":"params = SimpleNamespace(\n    window_size = 7,\n    batch_size = 2048,\n    epochs = 4,\n    geo_dim = 515,\n    hidden_dim = 515,\n    prod_dim = 60,\n    train = True\n)\nnum_heads = 5","metadata":{"papermill":{"duration":0.02282,"end_time":"2021-02-17T13:02:53.767719","exception":false,"start_time":"2021-02-17T13:02:53.744899","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.705049Z","iopub.execute_input":"2022-05-01T02:32:05.705416Z","iopub.status.idle":"2022-05-01T02:32:05.711688Z","shell.execute_reply.started":"2022-05-01T02:32:05.705383Z","shell.execute_reply":"2022-05-01T02:32:05.710450Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Select device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    print(\"WARNING: Training without GPU can be very slow!\")","metadata":{"papermill":{"duration":0.390934,"end_time":"2021-02-17T13:02:54.460792","exception":false,"start_time":"2021-02-17T13:02:54.069858","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.713572Z","iopub.execute_input":"2022-05-01T02:32:05.714244Z","iopub.status.idle":"2022-05-01T02:32:05.725569Z","shell.execute_reply.started":"2022-05-01T02:32:05.714168Z","shell.execute_reply":"2022-05-01T02:32:05.724117Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"n_sku = len(sku)\nn_geo = len(geo_params)\n\n# Transforms a given SKU to index\ndict_sku = {}\nfor i_sku in range(n_sku):\n    sku_id = sku['SKU'][i_sku]\n    dict_sku[sku_id] = i_sku\n    \n# Transforms a given geo_Cluster to index\ndict_geo = {}\nfor i_geo in range(n_geo):\n    geo_id = geo_params['geoCluster'][i_geo]\n    dict_geo[geo_id] = i_geo","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:32:05.729218Z","iopub.execute_input":"2022-05-01T02:32:05.729915Z","iopub.status.idle":"2022-05-01T02:32:05.746915Z","shell.execute_reply.started":"2022-05-01T02:32:05.729868Z","shell.execute_reply":"2022-05-01T02:32:05.745750Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def get_day(sku_id, date_str):\n    \n    sk = dict_sku[int(sku_id)]\n    try:\n        return sales_mat[date_str,sk,:,:].flatten()\n    except:\n        return np.zeros(params.geo_dim)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:32:05.750737Z","iopub.execute_input":"2022-05-01T02:32:05.751444Z","iopub.status.idle":"2022-05-01T02:32:05.758478Z","shell.execute_reply.started":"2022-05-01T02:32:05.751317Z","shell.execute_reply":"2022-05-01T02:32:05.757204Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def batch_generator(idata, target, batch_size, shuffle=True):\n    nsamples = len(idata)\n    if shuffle:\n        perm = np.random.permutation(nsamples)\n    else:\n        perm = range(nsamples)\n\n    for i in range(0, nsamples, batch_size):\n        batch_idx = perm[i:i+batch_size]\n        if target is not None:\n            yield idata[batch_idx], target[batch_idx]\n        else:\n            yield idata[batch_idx], None","metadata":{"papermill":{"duration":0.023629,"end_time":"2021-02-17T13:02:53.889028","exception":false,"start_time":"2021-02-17T13:02:53.865399","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.760824Z","iopub.execute_input":"2022-05-01T02:32:05.761459Z","iopub.status.idle":"2022-05-01T02:32:05.772999Z","shell.execute_reply.started":"2022-05-01T02:32:05.761408Z","shell.execute_reply":"2022-05-01T02:32:05.771862Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def lossfunction (pred, real):\n    for i in range (len(pred)):\n        for j in range(len(pred[0])):\n            if np.isnan(real[i][j].cpu()):\n                real[i][j] = pred[i][j]\n    loss = torch.mean((real-pred)**2)\n    return loss","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:32:05.774791Z","iopub.execute_input":"2022-05-01T02:32:05.775225Z","iopub.status.idle":"2022-05-01T02:32:05.787126Z","shell.execute_reply.started":"2022-05-01T02:32:05.775178Z","shell.execute_reply":"2022-05-01T02:32:05.785940Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer, idata, target, batch_size, device, log=False):\n    model.train()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    for X, y in batch_generator(idata, target, batch_size, shuffle=True):\n        # Get input and target sequences from batch\n        X = torch.tensor(X, dtype=torch.long, device=device)\n        y = torch.tensor(y, dtype=torch.long, device=device)\n\n        model.zero_grad()\n        output = model(X)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        \n        # Training statistics\n        total_loss += loss.item()\n        niterations += 1\n        if niterations == 200 or niterations == 500 or niterations % 1000 == 0:\n            print(f'MSE={loss:.2f}')\n\n    print(f'TOTAL MSE={total_loss:.2f}')\n    return total_loss","metadata":{"papermill":{"duration":0.027336,"end_time":"2021-02-17T13:02:53.97072","exception":false,"start_time":"2021-02-17T13:02:53.943384","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.789390Z","iopub.execute_input":"2022-05-01T02:32:05.789829Z","iopub.status.idle":"2022-05-01T02:32:05.804049Z","shell.execute_reply.started":"2022-05-01T02:32:05.789746Z","shell.execute_reply":"2022-05-01T02:32:05.802795Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def validate(model, criterion, idata, target, batch_size, device):\n    model.eval()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    y_pred = []\n    with torch.no_grad():\n        for X, y in batch_generator(idata, target, batch_size, shuffle=False):\n            # Get input and target sequences from batch\n            X = torch.tensor(X, dtype=torch.long, device=device)\n            output = model(X)\n            if target is not None:\n                y = torch.tensor(y, dtype=torch.long, device=device)\n                loss = criterion(output, y)\n                total_loss += loss.item()\n                niterations += 1\n            else:\n                pred = torch.max(output, 1)[1].detach().to('cpu').numpy()\n                y_pred.append(pred)\n\n    if target is not None:\n        return total_loss\n    else:\n        return np.concatenate(y_pred)","metadata":{"papermill":{"duration":0.027855,"end_time":"2021-02-17T13:02:54.014134","exception":false,"start_time":"2021-02-17T13:02:53.986279","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.806078Z","iopub.execute_input":"2022-05-01T02:32:05.806962Z","iopub.status.idle":"2022-05-01T02:32:05.820704Z","shell.execute_reply.started":"2022-05-01T02:32:05.806914Z","shell.execute_reply":"2022-05-01T02:32:05.819367Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def attention(q, k, v, mask=None, dropout=None):\n    d_k = q.size()[-1]\n    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n    attn_logits = attn_logits / math.sqrt(d_k)\n    if mask is not None:\n        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n    attention = F.softmax(attn_logits, dim=-1)\n    values = torch.matmul(attention, v)\n    return values, attention","metadata":{"papermill":{"duration":0.026935,"end_time":"2021-02-17T13:03:23.615794","exception":false,"start_time":"2021-02-17T13:03:23.588859","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.824962Z","iopub.execute_input":"2022-05-01T02:32:05.825350Z","iopub.status.idle":"2022-05-01T02:32:05.836094Z","shell.execute_reply.started":"2022-05-01T02:32:05.825276Z","shell.execute_reply":"2022-05-01T02:32:05.834908Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class SelfMultiAttention(nn.Module):\n    def __init__(self, geo_dim, num_heads, hidden_dim = None, bias=True):\n        super().__init__()\n        if hidden_dim is None:\n            hidden_dim = geo_dim\n        assert geo_dim % num_heads == 0, \"Geo dimension must be 0 modulo number of heads.\"\n        self.num_heads = num_heads\n        self.head_dim = geo_dim // num_heads\n        self.qkv_proj = nn.Linear(geo_dim, 3*geo_dim,bias=bias)\n        self.k_proj = nn.Linear(geo_dim, geo_dim, bias=bias)\n        self.v_proj = nn.Linear(geo_dim, geo_dim, bias=bias)\n        self.q_proj = nn.Linear(geo_dim, geo_dim, bias=bias)\n        self.out_proj = nn.Linear(geo_dim, hidden_dim, bias=bias)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # Empirically observed the convergence to be much better with the scaled initialization\n        nn.init.xavier_uniform_(self.k_proj.weight)\n        self.k_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.v_proj.weight)\n        self.v_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.q_proj.weight)\n        self.q_proj.bias.data.fill_(0)\n        nn.init.xavier_uniform_(self.out_proj.weight)\n        self.out_proj.bias.data.fill_(0)\n        if self.out_proj.bias is not None:\n            nn.init.constant_(self.out_proj.bias, 0.)\n\n    def forward(self, x, mask = None):\n        seq_length = x.size()[1]\n        qkv = self.qkv_proj(x)\n        \n        try:\n            qkv = qkv.reshape(params.batch_size, seq_length, self.num_heads, 3*self.head_dim)\n        except:\n            batch_size = qkv.size()[0]\n            qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n            \n        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n        q, k, v = qkv.chunk(3, dim=-1)\n        y, _ = attention(q, k, v, mask = mask) #Attention layer\n        y = y.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n        \n        try: \n            y = y.reshape(params.batch_size, seq_length , params.geo_dim)\n        except:\n            batch_size = y.size()[0]\n            y = y.reshape(batch_size, seq_length , params.geo_dim)\n            \n        y = self.out_proj(y)\n        return y","metadata":{"papermill":{"duration":0.030125,"end_time":"2021-02-17T13:03:23.696817","exception":false,"start_time":"2021-02-17T13:03:23.666692","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.838464Z","iopub.execute_input":"2022-05-01T02:32:05.838916Z","iopub.status.idle":"2022-05-01T02:32:05.859161Z","shell.execute_reply.started":"2022-05-01T02:32:05.838873Z","shell.execute_reply":"2022-05-01T02:32:05.857888Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class TransformerLayer(nn.Module):\n    def __init__(self, d_model, num_heads, dim_feedforward=512, dropout=0.1, activation=\"relu\"):\n        super().__init__()\n        self.self_attn = SelfMultiAttention(d_model,num_heads)\n        \n        # Implementation of Feedforward model\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout1 = nn.Dropout(dropout)\n        self.dropout2 = nn.Dropout(dropout)\n\n    def forward(self, src):\n        src2 = self.self_attn(src)\n        src = src + self.dropout1(src2)\n        src = self.norm1(src)\n        src2 = self.linear2(self.dropout(F.relu(self.linear1(src))))\n        src = src + self.dropout2(src2)\n        src = self.norm2(src)\n        return src","metadata":{"papermill":{"duration":0.028497,"end_time":"2021-02-17T13:03:23.742117","exception":false,"start_time":"2021-02-17T13:03:23.71362","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.861544Z","iopub.execute_input":"2022-05-01T02:32:05.862231Z","iopub.status.idle":"2022-05-01T02:32:05.877422Z","shell.execute_reply.started":"2022-05-01T02:32:05.862182Z","shell.execute_reply":"2022-05-01T02:32:05.876108Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class Predictor(nn.Module):\n    def __init__(self, geo_dim, prod_dim, num_heads, get_day = get_day, hidden_dim = None, context_dates=params.window_size):\n        super().__init__()\n        if hidden_dim is None:\n            hidden_dim = geo_dim\n        self.get_day = get_day\n        self.num_heads = num_heads\n        self.lin = nn.Linear(hidden_dim + prod_dim, geo_dim, bias=False)\n        self.att = TransformerLayer(hidden_dim,num_heads)\n        self.position_embedding = nn.Parameter(torch.Tensor(context_dates, geo_dim))\n        nn.init.xavier_uniform_(self.position_embedding)\n\n    def forward(self, inp):\n        d = []\n        z = torch.zeros((inp.shape[0],60)).cuda()\n        for i, batch in enumerate(inp):\n            b = []\n            for sku, date in batch:\n                b.append(self.get_day(sku,date))\n            d.append(b)\n            z[i][dict_sku[int(sku)]] = 1\n\n        d = torch.Tensor(d)\n        u = d.cuda() + self.position_embedding\n        v = self.att(u) #Transformer layer\n        x = v.sum(dim = 1)\n        c = torch.cat((x,z),dim = 1)\n        y = self.lin(c)\n        return y","metadata":{"papermill":{"duration":0.027797,"end_time":"2021-02-17T13:03:23.786956","exception":false,"start_time":"2021-02-17T13:03:23.759159","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:05.879748Z","iopub.execute_input":"2022-05-01T02:32:05.880193Z","iopub.status.idle":"2022-05-01T02:32:05.894606Z","shell.execute_reply.started":"2022-05-01T02:32:05.880147Z","shell.execute_reply":"2022-05-01T02:32:05.893348Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"min_date = sales.groupby([\"SKU\"]).min()\nmax_date = sales.groupby([\"SKU\"]).max()\ninp = [[],[]]\nout = [[],[]]\nfor s in sku['SKU']:\n    date_min = min_date[min_date.index == s]\n    date_max = max_date[max_date.index == s]\n    ms = list(date_min['date'])[0].split(\"-\")\n    min_s = datetime.datetime(int(ms[0]),int(ms[1]),int(ms[2]))\n    \n    ms = list(date_max['date'])[0].split(\"-\")\n    max_s = datetime.datetime(int(ms[0]),int(ms[1]),int(ms[2]))\n    \n    diff = (max_s - min_s).days\n    d = min_s\n    \n    #The earliest date found in the dataset is the 2020/01/01 used to calculate the index\n    while d + datetime.timedelta(7) < max_s - datetime.timedelta(diff // 10):\n        inp[0].append(np.array([(s,(d + datetime.timedelta(i) - datetime.datetime(2020,1,1)).days) for i in range (7)]))\n        out[0].append(get_day(s,(d + datetime.timedelta(7) - datetime.datetime(2020,1,1)).days))\n        d += datetime.timedelta(1)\n        \n    while d + datetime.timedelta(7) < max_s:\n        inp[1].append(np.array([(s,(d + datetime.timedelta(i) - datetime.datetime(2020,1,1)).days) for i in range (7)]))\n        out[1].append(get_day(s,(d + datetime.timedelta(7) - datetime.datetime(2020,1,1)).days))\n        d += datetime.timedelta(1)\ndata = np.array([[inp[0],out[0]], [inp[1],out[1]]])","metadata":{"execution":{"iopub.status.busy":"2022-05-01T02:32:05.896617Z","iopub.execute_input":"2022-05-01T02:32:05.897547Z","iopub.status.idle":"2022-05-01T02:32:11.491889Z","shell.execute_reply.started":"2022-05-01T02:32:05.897324Z","shell.execute_reply":"2022-05-01T02:32:11.488579Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model = Predictor(params.geo_dim,params.prod_dim,num_heads).to(device)","metadata":{"papermill":{"duration":4.332795,"end_time":"2021-02-17T13:03:28.136905","exception":false,"start_time":"2021-02-17T13:03:23.80411","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:11.493859Z","iopub.execute_input":"2022-05-01T02:32:11.494198Z","iopub.status.idle":"2022-05-01T02:32:11.539198Z","shell.execute_reply.started":"2022-05-01T02:32:11.494140Z","shell.execute_reply":"2022-05-01T02:32:11.538121Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(model)\nfor name, param in model.named_parameters():\n    print(f'{name:20} {param.numel()} {list(param.shape)}')\nprint(f'TOTAL                {sum(p.numel() for p in model.parameters())}')","metadata":{"papermill":{"duration":0.030823,"end_time":"2021-02-17T13:03:28.185171","exception":false,"start_time":"2021-02-17T13:03:28.154348","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:11.540911Z","iopub.execute_input":"2022-05-01T02:32:11.541712Z","iopub.status.idle":"2022-05-01T02:32:11.557205Z","shell.execute_reply.started":"2022-05-01T02:32:11.541636Z","shell.execute_reply":"2022-05-01T02:32:11.555910Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"## Train and Validation\nIn this section the model is trained with 90% of the dataset and validated with the remaining 10% dataset. The train and validation partition is generated over different products. The data variable contains the dataset partition, the first position contains the train partition whether the second position contains the validation data. \n\nThe performance of the model is measured with the MSE.","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters())\ncriterion = lossfunction\n\ntrain_accuracy = []\nvalid_accuracy = []\n\nfor epoch in range(params.epochs):\n    loss = train(model, criterion, optimizer, np.array(data[0][0]), np.array(data[0][1]), params.batch_size, device, log=True)\n    print(f'| epoch {epoch:03d} | train loss={loss:.2f}')\n    loss = validate(model, criterion, np.array(data[1][0]), np.array(data[1][1]), params.batch_size, device)\n    print(f'| epoch {epoch:03d} | valid loss={loss:.2f}')\n","metadata":{"papermill":{"duration":13330.124546,"end_time":"2021-02-17T16:45:38.327606","exception":false,"start_time":"2021-02-17T13:03:28.20306","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T02:32:11.559333Z","iopub.execute_input":"2022-05-01T02:32:11.559672Z","iopub.status.idle":"2022-05-01T03:35:35.330563Z","shell.execute_reply.started":"2022-05-01T02:32:11.559614Z","shell.execute_reply":"2022-05-01T03:35:35.329518Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Test","metadata":{}},{"cell_type":"markdown","source":"In order to see the real application of the predictor algorithm the model must be ran over the test data.","metadata":{}},{"cell_type":"code","source":"test = test.sort_values('date')\ntest_data = []\ntest_info = []\nfor r in test.iterrows():\n    test_sku = r[1]['SKU']\n    ms = r[1]['date'].split('-')\n    test_date = datetime.datetime(int(ms[0]),int(ms[1]),int(ms[2]))\n    week = np.array([(test_sku,(test_date - datetime.timedelta(i) - datetime.datetime(2020,1,1)).days) for i in range (7,0,-1)])\n    test_data.append(np.array(week))\n    test_info.append((test_sku, (test_date - datetime.datetime(2020,1,1)).days, dict_geo[r[1]['geoCluster']]))\ntest_data = np.array(test_data)","metadata":{"papermill":{"duration":0.402733,"end_time":"2021-02-17T16:45:38.811483","exception":false,"start_time":"2021-02-17T16:45:38.40875","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-05-01T03:35:35.332374Z","iopub.execute_input":"2022-05-01T03:35:35.332906Z","iopub.status.idle":"2022-05-01T03:36:01.294999Z","shell.execute_reply.started":"2022-05-01T03:35:35.332845Z","shell.execute_reply":"2022-05-01T03:36:01.294026Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## Filling the test file","metadata":{}},{"cell_type":"markdown","source":"The prediction the sales is added to the final array in batches in order to optimize the memory usage.","metadata":{}},{"cell_type":"code","source":"pred_sales = []\nwith torch.no_grad():\n    for i in range (0,len(test_data),100):\n        pred = model(test_data[i : i+100])\n        for p, g in zip(pred,test_info[i:i+100]):\n            sales_mat[g[1],dict_sku[g[0]],:] = p.cpu()\n            pred_sales.append(max(0,float(p[g[2]])))\ntest['sales'] = pred_sales","metadata":{"execution":{"iopub.status.busy":"2022-05-01T03:36:01.296522Z","iopub.execute_input":"2022-05-01T03:36:01.296818Z","iopub.status.idle":"2022-05-01T03:38:07.005527Z","shell.execute_reply.started":"2022-05-01T03:36:01.296777Z","shell.execute_reply":"2022-05-01T03:38:07.004541Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test = test.sort_values(['geoCluster', 'SKU'])\ntest.to_csv(\"test_test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-05-01T03:38:07.006853Z","iopub.execute_input":"2022-05-01T03:38:07.007373Z","iopub.status.idle":"2022-05-01T03:38:08.127730Z","shell.execute_reply.started":"2022-05-01T03:38:07.007329Z","shell.execute_reply":"2022-05-01T03:38:08.126657Z"},"trusted":true},"execution_count":39,"outputs":[]}]}